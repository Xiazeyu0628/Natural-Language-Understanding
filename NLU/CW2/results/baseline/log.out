INFO: COMMAND: train.py --save-dir /Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/results/baseline --log-file /Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/results/baseline/log.out --data /Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/europarl_prepared --translate-log-file /Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/results/baseline/translate.out --arch lstm
INFO: Arguments: {'data': '/Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/europarl_prepared', 'source_lang': 'de', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 10, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 100, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 10, 'log_file': '/Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/results/baseline/log.out', 'translate_log_file': '/Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/results/baseline/translate.out', 'save_dir': '/Users/xiazeyu/Documents/GitHub/Natural-Language-Understanding/NLU/CW2/results/baseline', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (de) with 5047 words
INFO: Loaded a target dictionary (en) with 4420 words
INFO: Built a model with 1456644 parameters
INFO: Epoch 000: loss 5.624 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 14.5 | clip 0.982 | train_perplexity 277
INFO: Epoch 000: valid_loss 5.09 | num_tokens 13.8 | batch_size 500 | valid_perplexity 163
INFO: Epoch 001: loss 5.058 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 15.5 | clip 1 | train_perplexity 157
INFO: Epoch 001: valid_loss 4.81 | num_tokens 13.8 | batch_size 500 | valid_perplexity 123
INFO: Epoch 002: loss 4.791 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 15.66 | clip 1 | train_perplexity 120
INFO: Epoch 002: valid_loss 4.59 | num_tokens 13.8 | batch_size 500 | valid_perplexity 98.4
